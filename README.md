
# ‚ö° Big Data - Apache Spark

Aplicando ci√™ncia de dados em cima de grandes volumes de dados atrav√©s do paradigma de divis√£o e conquista com Apache Spark




## üíª Ambiente

Fizemos nosso trabalho usando dois ambientes distintos: um ambiente local e o Google Colab. Optamos por essa abordagem pois n√£o tinhamos recursos suficientes para o processamento de dados dos tweets.

No ambiente local, configuramos nosso pr√≥prio ambiente de desenvolvimento Spark em nossas m√°quinas. Usamos esse ambiente principalmente para a an√°lise das reviews, pois pod√≠amos acessar facilmente os conjuntos de dados locais e executar nossos scripts Spark diretamente em nossas m√°quinas.

Por outro lado, usamos o Google Colab para a campanha pol√≠tica. Optamos por essa plataforma porque ela oferece recursos computacionais poderosos e uma integra√ß√£o f√°cil com o Spark. Como a campanha pol√≠tica envolvia o processamento de grandes volumes de dados em escala, o ambiente do Google Colab nos proporcionou os recursos necess√°rios para lidar com essa demanda computacional.

Al√©m disso, utilizamos os seguintes recursos/ferramentas:
 - Python
 - Jupyter
 - Pyspark
 - Pandas
 - Matplotlib
 - Seaborn
 - Regex
 - Natural Language Toolkit
 - TextBlob

## üê§ Tweets da Campanha Eleitoral de 2014

### [Hashtags e Tweets](https://github.com/thomz2/big-data-spark/blob/main/analysis/election_campaign/hashtags_and_tweets.ipynb)

#### Hashtags mais usadas pela manh√£, tarde e noite:
Para identificar as hashtags mais usadas pela manh√£, tarde e noite, fazemos uma an√°lise temporal dos tweets, categorizando-os em diferentes per√≠odos do dia (manh√£, tarde e noite) com base no hor√°rio em que foram postados. 

Em seguida, fazemos a contagem das hashtags mais frequentes em cada per√≠odo e os resultados s√£o apresentados em gr√°ficos de barras, mostrando as 10 hashtags mais populares em cada per√≠odo do dia:
![image](https://github.com/thomz2/big-data-spark/assets/82160387/29461700-ba80-45b4-baf8-4087a305f49c)
![image](https://github.com/thomz2/big-data-spark/assets/82160387/5ddfe7f3-a70f-416b-8c00-18ca3e588376)
![image](https://github.com/thomz2/big-data-spark/assets/82160387/0f917453-105d-469b-a376-456edfed091a)

#### Hashtags mais usadas em cada dia
A an√°lise das hashtags mais usadas em cada dia √© feita atrav√©s da contagem das hashtags presentes nos tweets de cada dia, agrupadas por data. 

Filtramos os tweets que possuem hashtags, contabilizamos a frequ√™ncia das hashtags em cada dia e apresentamos os resultados em gr√°ficos de barras, exibindo as 10 hashtags mais utilizadas em cada dia:
![image](https://github.com/thomz2/big-data-spark/assets/82160387/2b854da1-dc38-4c8e-a5be-6aba4a79162d)
![image](https://github.com/thomz2/big-data-spark/assets/82160387/a3f41341-509b-4a8b-a051-173957c4f476)
![image](https://github.com/thomz2/big-data-spark/assets/82160387/27678806-1865-4a5c-84c1-c39923557c22)
![image](https://github.com/thomz2/big-data-spark/assets/82160387/7f1d44db-382c-4474-a63b-4313c19eb32a)
![image](https://github.com/thomz2/big-data-spark/assets/82160387/5a004644-d180-4a98-9e6b-0ac750ded28a)
![image](https://github.com/thomz2/big-data-spark/assets/82160387/35314cc2-10ad-475c-ac88-dc6ba5c07645)

#### N√∫mero de tweets por hora a cada dia
Calculamos o n√∫mero de tweets por hora em cada dia, agrupando os tweets por hora do dia e contabilizando a quantidade de tweets em cada per√≠odo. 

Em seguida, apresentamos os resultados em um gr√°fico de linha, mostrando a distribui√ß√£o temporal do n√∫mero de tweets ao longo das horas do dia:

![image](https://github.com/thomz2/big-data-spark/assets/82160387/291ff125-d9f3-4d74-9e5e-23d77483e5a7)

### Figuras Pol√≠ticas

#### [Principais senten√ßas relacionadas √† palavra ‚ÄúDilma‚Äù](https://github.com/thomz2/big-data-spark/blob/main/analysis/election_campaign/dilma.ipynb)
Para identificar as principais senten√ßas relacionadas √† palavra "Dilma", carregamos os tweets relacionados √† campanha eleitoral do dataset e filtramos aqueles que cont√™m a palavra-chave "Dilma", ap√≥s isso, realizamos a limpeza e formata√ß√£o dos dados, removendo caracteres especiais e stopwords da l√≠ngua portuguesa. 

Ap√≥s essa etapa, utilizamos a t√©cnica de n-gramas para identificar sequ√™ncias de 3 a 5 palavras mais frequentes nos tweets, selecionando dessas, as 10 express√µes mais comuns:
![image](https://github.com/thomz2/big-data-spark/assets/82160387/c0fe029f-2e82-4b78-91e6-037300b1ec35)

#### [Principais senten√ßas relacionadas √† palavra ‚ÄúA√©cio‚Äù](https://github.com/thomz2/big-data-spark/blob/main/analysis/election_campaign/aecio.ipynb)
Utilizamos o mesmo processo para identificarmos as principais senten√ßas relacionadas √† palavra "A√©cio".

Fizemos isso apenas trocando a constante `WORD` que era `dilma` para `aecio`:
![image](https://github.com/thomz2/big-data-spark/assets/82160387/6435e1d6-1e4f-468a-99be-58ae1f3c6eae)

## üóº Reviews Relacionados √† Visita da Torre Eiffel em Paris

### [Palavras e Express√µes](https://github.com/thomz2/big-data-spark/blob/main/analysis/eiffel_tower/most_frequent_words_and_expressions.ipynb)

#### Palavras mais utilizadas nas avalia√ß√µes
Para encontrar as palavras mais utilizadas nas avalia√ß√µes, n√≥s seguimos um processo sistem√°tico de limpeza e an√°lise de texto. Primeiro, carregamos os dados das avalia√ß√µes, focando nas colunas de t√≠tulo e texto. Removemos quaisquer valores ausentes para garantir a integridade dos dados. Em seguida, realizamos a limpeza das colunas, removendo caracteres especiais e mantendo apenas letras e n√∫meros.

Depois disso, tokenizamos o texto, transformando as frases em listas de palavras individuais. Para aumentar a precis√£o da an√°lise, removemos as palavras comuns (stop words) que geralmente n√£o carregam muito significado, como "e", "de", "a", etc. Em seguida, usamos a fun√ß√£o explode para separar cada palavra em uma linha pr√≥pria. Contamos a frequ√™ncia de cada palavra usando a fun√ß√£o groupBy e ordenamos os resultados em ordem decrescente de frequ√™ncia. Por fim, plotamos as 10 palavras mais frequentes em um gr√°fico de barras:
![image](https://github.com/thomz2/big-data-spark/assets/82160387/7b917330-c775-4654-958d-c27e8a47cfce)

#### Express√µes mais usadas nas avalia√ß√µes
Para encontrar as express√µes mais usadas nas avalia√ß√µes, come√ßamos com o mesmo processo de limpeza e tokeniza√ß√£o usado para as palavras individuais. Em seguida, criamos n-grams para identificar express√µes comuns.

Depois de gerar essas express√µes, explodimos as listas de n-grams, assim como fizemos com as palavras individuais, para que cada express√£o tivesse sua pr√≥pria linha. Agrupamos essas express√µes e contamos a frequ√™ncia de cada uma, ordenando-as em ordem decrescente de contagem. Combinamos os resultados de n-grams de textos e t√≠tulos, agrupamos novamente e somamos as frequ√™ncias para obter as express√µes mais frequentes no conjunto de dados completo. Finalmente, apresentamos as 10 express√µes mais usadas em um gr√°fico de barras:
![image](https://github.com/thomz2/big-data-spark/assets/82160387/55df194b-b0f8-4c25-aaab-08006ca18354)

#### Principais t√≥picos relacionados √†s revis√µes
Para encontrar os principais t√≥picos relacionados √†s revis√µes, come√ßamos lematizando as palavras do texto. Utilizamos a biblioteca NLTK para isso, especialmente o WordNetLemmatizer, que reduz as palavras √† sua forma base. Em seguida, organizamos as palavras lematizadas em uma lista de palavras. 

Ap√≥s essa etapa, vetorizamos essas palavras utilizando o CountVectorizer do Spark, transformando-as em caracter√≠sticas vetoriais. Ent√£o, aplicamos o modelo de An√°lise de T√≥picos Latentes (LDA) para identificar os t√≥picos mais relevantes presentes nos dados de revis√µes. Finalmente, mapeamos os termos mais importantes de cada t√≥pico de volta para as palavras originais usando a fun√ß√£o topic_words e exibimos esses resultados em um DataFrame do Pandas para uma melhor visualiza√ß√£o e an√°lise:
![image](https://github.com/thomz2/big-data-spark/assets/82160387/2d8eb776-2b72-48ee-b1db-d68f6afd3a11)

### [Distribui√ß√µes](https://github.com/thomz2/big-data-spark/blob/main/analysis/eiffel_tower/distributions.ipynb)

#### Distribui√ß√£o temporal das revis√µes
Para mapearmos a distribui√ß√£o temporal das revis√µes, come√ßamos selecionando a coluna createdAt do DataFrame e removendo valores nulos. Ajustamos a pol√≠tica do parser de datas do Spark para "legacy" (importante) e convertemos as datas para o formato yyyy-MM, criando a coluna year_month. Em seguida, agrupamos as revis√µes por year_month e contamos o n√∫mero de revis√µes para cada per√≠odo, organizando os dados em ordem crescente.

Com os dados agrupados, utilizamos seaborn e matplotlib para criar um gr√°fico de linhas que mostra a contagem de revis√µes ao longo do tempo. Esse gr√°fico revela padr√µes e sazonalidades nas revis√µes da Torre Eiffel, permitindo-nos identificar per√≠odos de maior ou menor atividade nos coment√°rios dos visitantes:
![image](https://github.com/thomz2/big-data-spark/assets/82160387/550ddfa4-fc76-4664-9ebc-9e4b6d325152)

#### Distribui√ß√£o dos sentimentos das revis√µes
Para analisar a distribui√ß√£o dos sentimentos das revis√µes, utilizamos TextBlob para calcular a polaridade do sentimento de cada texto de revis√£o. Extra√≠mos a coluna text do DataFrame, limpamos os dados convertendo para min√∫sculas e removendo caracteres n√£o alfab√©ticos, e aplicamos uma fun√ß√£o UDF para determinar a polaridade, que varia de -1 (negativo) a 1 (positivo).

Convertemos o DataFrame resultante para um DataFrame do Pandas e criamos um histograma, o qual nos permite entender melhor a predomin√¢ncia de sentimentos positivos, negativos ou neutros nas reviews:
![image](https://github.com/thomz2/big-data-spark/assets/82160387/c84a36f2-8cde-4eeb-b192-ec2c5ea84af5)
