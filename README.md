
# ‚ö° Big Data - Apache Spark

Aplicando ci√™ncia de dados em cima de grandes volumes de dados atrav√©s do paradigma de divis√£o e conquista com Apache Spark




## üíª Ambiente

Fizemos nosso trabalho usando dois ambientes distintos: um ambiente local e o Google Colab. Optamos por essa abordagem pois n√£o tinhamos recursos suficientes para o processamento de dados dos tweets.

No ambiente local, configuramos nosso pr√≥prio ambiente de desenvolvimento Spark em nossas m√°quinas. Usamos esse ambiente principalmente para a an√°lise das reviews, pois pod√≠amos acessar facilmente os conjuntos de dados locais e executar nossos scripts Spark diretamente em nossas m√°quinas.

Por outro lado, usamos o Google Colab para a campanha pol√≠tica. Optamos por essa plataforma porque ela oferece recursos computacionais poderosos e uma integra√ß√£o f√°cil com o Spark. Como a campanha pol√≠tica envolvia o processamento de grandes volumes de dados em escala, o ambiente do Google Colab nos proporcionou os recursos necess√°rios para lidar com essa demanda computacional.

Al√©m disso, utilizamos os seguintes recursos/ferramentas:
 - Python
 - Jupyter
 - Pyspark
 - Pandas
 - Matplotlib
 - Seaborn
 - Regex
 - Natural Language Toolkit
 - TextBlob

## üê§ Tweets da Campanha Eleitoral de 2014

### Hashtags e Tweets

#### Hashtags mais usadas pela manh√£, tarde e noite:
Para identificar as hashtags mais usadas pela manh√£, tarde e noite, fazemos uma an√°lise temporal dos tweets, categorizando-os em diferentes per√≠odos do dia (manh√£, tarde e noite) com base no hor√°rio em que foram postados. 

Em seguida, fazemos a contagem das hashtags mais frequentes em cada per√≠odo e os resultados s√£o apresentados em gr√°ficos de barras, mostrando as 10 hashtags mais populares em cada per√≠odo do dia:

#### Hashtags mais usadas em cada dia
A an√°lise das hashtags mais usadas em cada dia √© feita atrav√©s da contagem das hashtags presentes nos tweets de cada dia, agrupadas por data. 

Filtramos os tweets que possuem hashtags, contabilizamos a frequ√™ncia das hashtags em cada dia e apresentamos os resultados em gr√°ficos de barras, exibindo as 10 hashtags mais utilizadas em cada dia:

#### N√∫mero de tweets por hora a cada dia
Calculamos o n√∫mero de tweets por hora em cada dia, agrupando os tweets por hora do dia e contabilizando a quantidade de tweets em cada per√≠odo. 

Em seguida, apresentamos os resultados em um gr√°fico de linha, mostrando a distribui√ß√£o temporal do n√∫mero de tweets ao longo das horas do dia:

### Figuras Pol√≠ticas

#### Principais senten√ßas relacionadas √† palavra ‚ÄúDilma‚Äù
Para identificar as principais senten√ßas relacionadas √† palavra "Dilma", carregamos os tweets relacionados √† campanha eleitoral do dataset e filtramos aqueles que cont√™m a palavra-chave "Dilma", ap√≥s isso, realizamos a limpeza e formata√ß√£o dos dados, removendo caracteres especiais e stopwords da l√≠ngua portuguesa. 

Ap√≥s essa etapa, utilizamos a t√©cnica de n-gramas para identificar sequ√™ncias de 3 a 5 palavras mais frequentes nos tweets, selecionando dessas, as 10 express√µes mais comuns:

#### Principais senten√ßas relacionadas √† palavra ‚ÄúA√©cio‚Äù
Utilizamos o mesmo processo para identificarmos as principais senten√ßas relacionadas √† palavra "A√©cio".

Fizemos isso apenas trocando a constante `WORD` que era `dilma` para `aecio`:

## üóº Reviews Relacionados √† Visita da Torre Eiffel em Paris

### Palavras e Express√µes

#### Palavras mais utilizadas nas avalia√ß√µes
Para encontrar as palavras mais utilizadas nas avalia√ß√µes, n√≥s seguimos um processo sistem√°tico de limpeza e an√°lise de texto. Primeiro, carregamos os dados das avalia√ß√µes, focando nas colunas de t√≠tulo e texto. Removemos quaisquer valores ausentes para garantir a integridade dos dados. Em seguida, realizamos a limpeza das colunas, removendo caracteres especiais e mantendo apenas letras e n√∫meros.

Depois disso, tokenizamos o texto, transformando as frases em listas de palavras individuais. Para aumentar a precis√£o da an√°lise, removemos as palavras comuns (stop words) que geralmente n√£o carregam muito significado, como "e", "de", "a", etc. Em seguida, usamos a fun√ß√£o explode para separar cada palavra em uma linha pr√≥pria. Contamos a frequ√™ncia de cada palavra usando a fun√ß√£o groupBy e ordenamos os resultados em ordem decrescente de frequ√™ncia. Por fim, plotamos as 10 palavras mais frequentes em um gr√°fico de barras:

#### Express√µes mais usadas nas avalia√ß√µes
Para encontrar as express√µes mais usadas nas avalia√ß√µes, come√ßamos com o mesmo processo de limpeza e tokeniza√ß√£o usado para as palavras individuais. Em seguida, criamos n-grams para identificar express√µes comuns.

Depois de gerar essas express√µes, explodimos as listas de n-grams, assim como fizemos com as palavras individuais, para que cada express√£o tivesse sua pr√≥pria linha. Agrupamos essas express√µes e contamos a frequ√™ncia de cada uma, ordenando-as em ordem decrescente de contagem. Combinamos os resultados de n-grams de textos e t√≠tulos, agrupamos novamente e somamos as frequ√™ncias para obter as express√µes mais frequentes no conjunto de dados completo. Finalmente, apresentamos as 10 express√µes mais usadas em um gr√°fico de barras:

#### Principais t√≥picos relacionados √†s revis√µes
Jaja sai

### Distribui√ß√µes

#### Distribui√ß√£o temporal das revis√µes
Para mapearmos a distribui√ß√£o temporal das revis√µes, come√ßamos selecionando a coluna createdAt do DataFrame e removendo valores nulos. Ajustamos a pol√≠tica do parser de datas do Spark para "legacy" (importante) e convertemos as datas para o formato yyyy-MM, criando a coluna year_month. Em seguida, agrupamos as revis√µes por year_month e contamos o n√∫mero de revis√µes para cada per√≠odo, organizando os dados em ordem crescente.

Com os dados agrupados, utilizamos seaborn e matplotlib para criar um gr√°fico de linhas que mostra a contagem de revis√µes ao longo do tempo. Esse gr√°fico revela padr√µes e sazonalidades nas revis√µes da Torre Eiffel, permitindo-nos identificar per√≠odos de maior ou menor atividade nos coment√°rios dos visitantes:

#### Distribui√ß√£o dos sentimentos das revis√µes
Para analisar a distribui√ß√£o dos sentimentos das revis√µes, utilizamos TextBlob para calcular a polaridade do sentimento de cada texto de revis√£o. Extra√≠mos a coluna text do DataFrame, limpamos os dados convertendo para min√∫sculas e removendo caracteres n√£o alfab√©ticos, e aplicamos uma fun√ß√£o UDF para determinar a polaridade, que varia de -1 (negativo) a 1 (positivo).

Convertemos o DataFrame resultante para um DataFrame do Pandas e criamos um histograma, o qual nos permite entender melhor a predomin√¢ncia de sentimentos positivos, negativos ou neutros nas reviews: