{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\danys\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "    import findspark\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import lower, regexp_replace, col, split, explode, count, sum as spark_sum, concat_ws, array, udf, collect_list, array_distinct, flatten, to_timestamp, substring, when, hour, size\n",
    "    from pyspark.sql.types import ArrayType, StringType\n",
    "    from pyspark.ml.feature import StopWordsRemover, Tokenizer, NGram, CountVectorizer\n",
    "    from pyspark.ml.clustering import LDA\n",
    "    import pandas as pd\n",
    "    from textblob import TextBlob\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from functools import reduce\n",
    "    import re\n",
    "except:\n",
    "    print(\"downloading stuff...\")\n",
    "    %pip install pyspark\n",
    "    %pip install pandas\n",
    "    %pip install textblob\n",
    "    %pip install matplotlib\n",
    "    %pip install seaborn\n",
    "    %pip install functools\n",
    "    %pip install nltk\n",
    "    %pip install findspark\n",
    "    import nltk\n",
    "    import findspark\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import lower, regexp_replace, col, split, explode, count, sum as spark_sum, concat_ws, array, udf, collect_list, array_distinct, flatten, to_timestamp, substring, when, hour, size\n",
    "    from pyspark.sql.types import ArrayType, StringType\n",
    "    from pyspark.ml.feature import StopWordsRemover, Tokenizer, NGram, CountVectorizer\n",
    "    from pyspark.ml.clustering import LDA\n",
    "    import pandas as pd\n",
    "    from textblob import TextBlob\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from functools import reduce\n",
    "    import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pandas_df(spark_df, limit = None, *columns):\n",
    "    if not columns:\n",
    "        if limit is not None:\n",
    "            return spark_df.limit(limit).toPandas()\n",
    "        else:\n",
    "            return spark_df.toPandas()\n",
    "    else:\n",
    "        return spark_df.select(*columns).limit(limit).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-------------------+-----------+\n",
      "|                id|                text|          time_info|hora_do_dia|\n",
      "+------------------+--------------------+-------------------+-----------+\n",
      "|522394422710136832|@anacddd verdade,...|1970-01-01 14:31:50|         14|\n",
      "|522394422806581248|              Que ñ*|1970-01-01 14:31:50|         14|\n",
      "|522394422731100160| Vou quebrar a Bruna|1970-01-01 14:31:50|         14|\n",
      "|522394422810783745|agora vou p segun...|1970-01-01 14:31:50|         14|\n",
      "|522394423137943553|Me sinto tão bem ...|1970-01-01 14:31:50|         14|\n",
      "|522394423188271104|Eu estou aqui, de...|1970-01-01 14:31:50|         14|\n",
      "|522394423238606848|Quando vai embora...|1970-01-01 14:31:50|         14|\n",
      "|522394423528022016|@paynecaralhudo k...|1970-01-01 14:31:50|         14|\n",
      "|522394423632875521|Conceição da Barr...|1970-01-01 14:31:50|         14|\n",
      "|522394424010362881| @Maniavato te amo ♥|1970-01-01 14:31:50|         14|\n",
      "|522394424048091138|Alg me curtindo rs ♡|1970-01-01 14:31:50|         14|\n",
      "|522394424010358784|@MiiluAA No, porq...|1970-01-01 14:31:50|         14|\n",
      "|522394423741906944|#EMABiggestFansJu...|1970-01-01 14:31:50|         14|\n",
      "|522394424568213505|@raizabatista dev...|1970-01-01 14:31:51|         14|\n",
      "|522394424920506368|Me senti ate d fe...|1970-01-01 14:31:51|         14|\n",
      "|522394424811458560|qual o sentido de...|1970-01-01 14:31:51|         14|\n",
      "|522394425029574656|I'm at Lava Rápid...|1970-01-01 14:31:51|         14|\n",
      "|522394425121841153|Fica comentando m...|1970-01-01 14:31:51|         14|\n",
      "|522394425461579777|\"odeio que me man...|1970-01-01 14:31:51|         14|\n",
      "|522394425960701952|CAMAMTEBABILONFRA...|1970-01-01 14:31:51|         14|\n",
      "+------------------+--------------------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('ElectionCampaignTweetsAnalysis') \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .config('spark.driver.memory', '2g') \\\n",
    "    .config('spark.network.timeout', '800s') \\\n",
    "    .config('spark.sql.shuffle.partitions', '2') \\\n",
    "    .config('spark.python.worker.reuse', 'false') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Minha cópia mais leve do arquivo\n",
    "df = spark.read.option('header', 'false').option('delimiter', '\\t').csv('../../datasets/debate_tweets_c.tsv') \\\n",
    "    .withColumnRenamed('_c0', 'id').withColumnRenamed('_c1', 'text').withColumnRenamed('_c7', 'time_info') \\\n",
    "    .select('id', 'text', 'time_info')\n",
    "\n",
    "df = df.withColumn(\"time_info\", to_timestamp(substring(df[\"time_info\"], 12, 8), \"HH:mm:ss\"))\n",
    "df = df.withColumn(\"hora_do_dia\", hour(df[\"time_info\"]))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDFs importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDFs importantes\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def categorizar_periodo(hora_do_dia):\n",
    "    if 5 <= hora_do_dia < 12:\n",
    "        return \"manha\"\n",
    "    elif 12 <= hora_do_dia < 18:\n",
    "        return \"tarde\"\n",
    "    else:\n",
    "        return \"noite\"\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def extrai_hashtags(text):\n",
    "    return re.findall(r'#(\\w+)', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+\n",
      "|             hashtag|periodo|count|\n",
      "+--------------------+-------+-----+\n",
      "|    EMABiggestFans1D|  tarde| 2798|\n",
      "|EMABiggestFansJus...|  tarde| 2712|\n",
      "|          QueroNoTVZ|  tarde|  156|\n",
      "|QueroMuitosSeguid...|  tarde|   77|\n",
      "|       VoteVampsVevo|  tarde|   69|\n",
      "|AguacateTraeDeNue...|  tarde|   53|\n",
      "|   SorrisoNoEncontro|  tarde|   45|\n",
      "|   FrasesProfessores|  tarde|   45|\n",
      "|        PediuTocouRD|  tarde|   40|\n",
      "|                 MPN|  tarde|   33|\n",
      "|EMABiggestFansAri...|  tarde|   32|\n",
      "|            CreoEnTi|  tarde|   29|\n",
      "|EMABiggestFansNic...|  tarde|   26|\n",
      "|    EMABiggetsFans1D|  tarde|   23|\n",
      "|                 EMA|  tarde|   23|\n",
      "|            BestLive|  tarde|   23|\n",
      "|           BrunoMars|  tarde|   21|\n",
      "|              trndnl|  tarde|   19|\n",
      "|            Encontro|  tarde|   17|\n",
      "|                Luan|  tarde|   15|\n",
      "+--------------------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = df.withColumn(\"hashtags\", extrai_hashtags(df[\"text\"])) \\\n",
    "    .withColumn(\"periodo\", categorizar_periodo(col('hora_do_dia'))) \\\n",
    "    .filter(size(col('hashtags')) > 0) \\\n",
    "    .withColumn('hashtag', explode(\"hashtags\")) \\\n",
    "    .select('hashtag', 'periodo') \\\n",
    "    .groupBy('hashtag', 'periodo').count().orderBy(col('count').desc())\n",
    "\n",
    "new_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
